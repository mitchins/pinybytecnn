#!/usr/bin/env python3
"""
Comprehensive benchmark: 1-layer PinyByteCNN vs TinyGrad across input lengths
Test performance on half window, full window, 2x window, 3x window sizes
"""

import time
import statistics
import json
import sys
import os
from typing import List, Dict, Any

# Add paths
sys.path.append('/Users/mitchellcurrie/Projects/PinyByteCNN')

from tinybytecnn.multi_layer_optimized import MultiLayerByteCNN
from tinybytecnn.utils import text_to_fixed_bytes


def load_weights(path: str) -> Dict[str, Any]:
    """Load weights from JSON file"""
    with open(path, 'r') as f:
        return json.load(f)


def tinygrad_predict(weights: Dict[str, Any], text: str, max_len: int) -> float:
    """TinyGrad prediction for 1-layer model"""
    try:
        from tinygrad.tensor import Tensor
    except Exception as e:
        return None  # TinyGrad not available

    idxs = text_to_fixed_bytes(text, max_len=max_len)

    # Embedding
    emb = Tensor(weights["embedding"])
    x = emb[idxs]

    # Prepare for conv2d
    T = len(idxs)
    E = len(weights["embedding"][0])
    x = x.transpose(0, 1).reshape(1, E, 1, T)

    # Conv1D + ReLU
    conv_w = weights["conv1_weight"]
    O = len(conv_w)
    I = len(conv_w[0])
    K = len(conv_w[0][0])
    pad = K // 2
    w = Tensor(conv_w).reshape(O, I, 1, K)
    b = Tensor(weights["conv1_bias"]).reshape(O)
    y = x.conv2d(w, b, stride=1, padding=(0, pad))
    y = y.relu()

    # Max pooling
    y = y.max(axis=3)
    while len(y.shape) > 1:
        y = y.squeeze()

    # Dense layers
    W1 = Tensor(weights["classifier_weight"])
    b1 = Tensor(weights["classifier_bias"])
    h = y.matmul(W1.T) + b1
    h = h.relu()

    W2 = Tensor(weights["output_weight"])
    b2 = Tensor(weights["output_bias"])
    z = h.matmul(W2.T) + b2
    p = z.sigmoid()
    
    return float(p.item())


def generate_test_texts(window_size: int = 512) -> Dict[str, str]:
    """Generate test texts of different lengths relative to window size"""
    
    base_text = "This is a sample text for benchmarking. It contains various words and phrases that might be considered toxic or non-toxic. We need to test performance across different input lengths to understand how the model scales. "
    
    # Calculate target lengths
    half_len = window_size // 2  # 256 bytes
    full_len = window_size        # 512 bytes  
    double_len = window_size * 2  # 1024 bytes
    triple_len = window_size * 3  # 1536 bytes
    
    texts = {}
    
    # Generate texts by repeating base text
    current_text = ""
    while len(current_text.encode('utf-8')) < triple_len + 100:
        current_text += base_text
    
    # Extract texts of target lengths
    texts["half_window"] = current_text[:half_len//2]  # Rough character estimate
    texts["full_window"] = current_text[:full_len//2]
    texts["double_window"] = current_text[:double_len//2]  
    texts["triple_window"] = current_text[:triple_len//2]
    
    # Add some varied content
    texts["half_window"] += " Short text for testing."
    texts["full_window"] += " This is a moderate length text for benchmarking performance."
    texts["double_window"] += " This is a longer text that exceeds the window size and will test how the model handles truncation or processing of extended content."
    texts["triple_window"] += " This is a very long text that significantly exceeds the window size, allowing us to test model performance on extended inputs that require more processing power."
    
    return texts


def benchmark_implementation(predict_fn, weights_or_model, texts: Dict[str, str], 
                           implementation_name: str, warmup: int = 3, runs: int = 10) -> Dict[str, Any]:
    """Benchmark a prediction implementation"""
    
    results = {}
    
    print(f"\nðŸ”¥ Benchmarking {implementation_name}")
    print("-" * 50)
    
    for text_type, text in texts.items():
        actual_bytes = len(text.encode('utf-8'))
        print(f"ðŸ“ {text_type:15s}: {actual_bytes:4d} bytes")
        
        # Warmup
        for _ in range(warmup):
            try:
                predict_fn(weights_or_model, text, 512)
            except:
                pass
        
        # Benchmark runs
        times = []
        predictions = []
        
        for _ in range(runs):
            start_time = time.perf_counter()
            try:
                pred = predict_fn(weights_or_model, text, 512)
                end_time = time.perf_counter()
                
                if pred is not None:
                    times.append((end_time - start_time) * 1000)  # Convert to ms
                    predictions.append(pred)
            except Exception as e:
                print(f"   âŒ Error: {e}")
                break
        
        if times:
            avg_time = statistics.mean(times)
            med_time = statistics.median(times) 
            min_time = min(times)
            max_time = max(times)
            std_time = statistics.stdev(times) if len(times) > 1 else 0
            avg_pred = statistics.mean(predictions) if predictions else 0
            
            rps = 1000 / avg_time if avg_time > 0 else 0
            
            results[text_type] = {
                'avg_ms': avg_time,
                'med_ms': med_time,
                'min_ms': min_time,
                'max_ms': max_time,
                'std_ms': std_time,
                'rps': rps,
                'prediction': avg_pred,
                'bytes': actual_bytes,
                'runs': len(times)
            }
            
            print(f"   â±ï¸  Avg: {avg_time:.2f}ms | Med: {med_time:.2f}ms | RPS: {rps:.1f} | Pred: {avg_pred:.4f}")
        else:
            results[text_type] = {
                'error': True,
                'bytes': actual_bytes
            }
            print(f"   âŒ Failed to benchmark")
    
    return results


def pinybytecnn_predict(model, text: str, max_len: int) -> float:
    """PinyByteCNN prediction wrapper"""
    return model.predict(text)


def tinygrad_predict_wrapper(weights: Dict[str, Any], text: str, max_len: int) -> float:
    """TinyGrad prediction wrapper"""
    return tinygrad_predict(weights, text, max_len)


def main():
    """Run comprehensive benchmark"""
    
    print("ðŸš€ 1-LAYER PINYBYTECNN vs TINYGRAD BENCHMARK")
    print("=" * 70)
    print("ðŸ“Š Testing performance across different input lengths")
    print("ðŸŽ¯ Window size: 512 bytes")
    
    # Load model and weights
    weights_path = "/Users/mitchellcurrie/Projects/tinygrad-offensive-detector/pinybytecnn_1layer_filtered.json"
    
    if not os.path.exists(weights_path):
        print(f"âŒ Weights file not found: {weights_path}")
        return
    
    weights = load_weights(weights_path)
    
    # Create PinyByteCNN model
    layers_config = [
        {"in_channels": 14, "out_channels": 32, "kernel_size": 3, "use_batch_norm": True}
    ]
    pinybytecnn_model = MultiLayerByteCNN(
        layers_config=layers_config,
        hidden_dim=64,
        max_len=512,
        vocab_size=256
    )
    pinybytecnn_model.load_weights_from_dict(weights)
    
    print("âœ… Models loaded successfully")
    
    # Generate test texts
    texts = generate_test_texts(window_size=512)
    
    print(f"\nðŸ“ Generated test texts:")
    for text_type, text in texts.items():
        byte_len = len(text.encode('utf-8'))
        char_len = len(text)
        print(f"   {text_type:15s}: {byte_len:4d} bytes, {char_len:4d} chars")
    
    # Benchmark PinyByteCNN
    pinybytecnn_results = benchmark_implementation(
        pinybytecnn_predict, 
        pinybytecnn_model, 
        texts, 
        "PinyByteCNN",
        warmup=5,
        runs=50
    )
    
    # Benchmark TinyGrad
    try:
        from tinygrad.tensor import Tensor
        tinygrad_available = True
    except ImportError:
        tinygrad_available = False
        print("\nâš ï¸  TinyGrad not available, skipping TinyGrad benchmark")
    
    if tinygrad_available:
        tinygrad_results = benchmark_implementation(
            tinygrad_predict_wrapper,
            weights,
            texts,
            "TinyGrad",
            warmup=3,
            runs=20  # Fewer runs for TinyGrad due to overhead
        )
    else:
        tinygrad_results = {}
    
    # Analysis and comparison
    print(f"\nðŸ“Š PERFORMANCE ANALYSIS")
    print("=" * 70)
    
    print(f"\n{'Text Type':15s} {'Bytes':>6s} {'PinyByteCNN':>12s} {'TinyGrad':>12s} {'Speedup':>8s}")
    print("-" * 65)
    
    for text_type in texts.keys():
        piny_result = pinybytecnn_results.get(text_type, {})
        tg_result = tinygrad_results.get(text_type, {})
        
        bytes_len = piny_result.get('bytes', 0)
        piny_rps = piny_result.get('rps', 0)
        tg_rps = tg_result.get('rps', 0)
        
        if piny_rps > 0 and tg_rps > 0:
            speedup = piny_rps / tg_rps
            speedup_str = f"{speedup:.1f}x"
        else:
            speedup_str = "N/A"
        
        piny_rps_str = f"{piny_rps:.1f}" if piny_rps > 0 else "FAIL"
        tg_rps_str = f"{tg_rps:.1f}" if tg_rps > 0 else "FAIL"
        
        print(f"{text_type:15s} {bytes_len:6d} {piny_rps_str:>12s} {tg_rps_str:>12s} {speedup_str:>8s}")
    
    # Scaling analysis
    print(f"\nðŸ“ˆ SCALING ANALYSIS")
    print("-" * 30)
    
    for impl_name, results in [("PinyByteCNN", pinybytecnn_results), ("TinyGrad", tinygrad_results)]:
        if not results:
            continue
            
        print(f"\n{impl_name}:")
        
        # Calculate scaling factors
        half_rps = results.get('half_window', {}).get('rps', 0)
        full_rps = results.get('full_window', {}).get('rps', 0)
        double_rps = results.get('double_window', {}).get('rps', 0)
        triple_rps = results.get('triple_window', {}).get('rps', 0)
        
        if half_rps > 0:
            print(f"   Half â†’ Full:   {(half_rps/full_rps):.2f}x" if full_rps > 0 else "   Half â†’ Full:   N/A")
            print(f"   Half â†’ Double: {(half_rps/double_rps):.2f}x" if double_rps > 0 else "   Half â†’ Double: N/A")  
            print(f"   Half â†’ Triple: {(half_rps/triple_rps):.2f}x" if triple_rps > 0 else "   Half â†’ Triple: N/A")
    
    # Accuracy comparison
    print(f"\nðŸŽ¯ ACCURACY COMPARISON")
    print("-" * 35)
    
    for text_type in texts.keys():
        piny_pred = pinybytecnn_results.get(text_type, {}).get('prediction', 0)
        tg_pred = tinygrad_results.get(text_type, {}).get('prediction', 0)
        
        if piny_pred > 0 and tg_pred > 0:
            diff = abs(piny_pred - tg_pred)
            rel_diff = (diff / max(piny_pred, 1e-8)) * 100
            status = "âœ… MATCH" if rel_diff < 1.0 else "âš ï¸ DIFF"
            print(f"{text_type:15s}: Î”={diff:.6f} ({rel_diff:.2f}%) {status}")
    
    # Save results
    benchmark_results = {
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'window_size': 512,
        'model_params': 7266,
        'test_texts': {k: len(v.encode('utf-8')) for k, v in texts.items()},
        'pinybytecnn_results': pinybytecnn_results,
        'tinygrad_results': tinygrad_results
    }
    
    results_file = f"1layer_benchmark_{int(time.time())}.json"
    with open(results_file, 'w') as f:
        json.dump(benchmark_results, f, indent=2)
    
    print(f"\nðŸ’¾ Results saved to: {results_file}")
    
    # Summary
    print(f"\nðŸ† BENCHMARK SUMMARY")
    print("=" * 30)
    
    if pinybytecnn_results.get('full_window', {}).get('rps', 0) > 0:
        piny_full_rps = pinybytecnn_results['full_window']['rps']
        piny_full_ms = pinybytecnn_results['full_window']['avg_ms']
        print(f"âœ… PinyByteCNN (512B): {piny_full_rps:.1f} RPS, {piny_full_ms:.2f}ms avg")
    
    if tinygrad_results.get('full_window', {}).get('rps', 0) > 0:
        tg_full_rps = tinygrad_results['full_window']['rps'] 
        tg_full_ms = tinygrad_results['full_window']['avg_ms']
        print(f"âœ… TinyGrad (512B):   {tg_full_rps:.1f} RPS, {tg_full_ms:.2f}ms avg")
    
    print(f"ðŸŽ¯ Model: 1-layer, 7,266 parameters, F1=0.8352")
    print(f"ðŸ“Š Ready for production deployment!")


if __name__ == "__main__":
    main()